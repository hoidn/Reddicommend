{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import numpy as np\n",
    "from pyspark.mllib.linalg.distributed import CoordinateMatrix\n",
    "from pyspark.mllib.linalg.distributed import MatrixEntry\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "minimal_fields = [ \n",
    "          StructField(\"author\", StringType(), True),\n",
    "          StructField(\"score\", LongType(), True),\n",
    "          StructField(\"controversiality\", LongType(), True),\n",
    "          StructField(\"subreddit\", StringType(), True)]\n",
    "sj = sqlContext.read.json(\"s3a://insight-ohoidn/sample3.json\", StructType(minimal_fields))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "occurrences = sqlContext.sql(\"\"\"\n",
    "select *, dense_rank() over (order by subreddit desc) as rid \n",
    "from (SELECT subreddit, author, sum(score) as tally,\\\n",
    "        sum(abs(score)) as activity, dense_rank() over (order by author desc) as uid\n",
    "    from test\n",
    "    group by subreddit, author)\n",
    "where tally!=0\n",
    "\"\"\").persist(StorageLevel.MEMORY_AND_DISK_SER)\n",
    "occurrences.registerTempTable('occurrences')\n",
    " \n",
    "coordinate_matrix = CoordinateMatrix(bare_occurrences.rdd.map(tuple))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from operator import add\n",
    "\n",
    "# TODO: check that zero entries are correctly filtered\n",
    "def coordinateMatrixMultiply(leftmat, rightmat):\n",
    "    m = leftmat.entries.map(lambda entry: (entry.j, (entry.i, entry.value)))\n",
    "    n = rightmat.entries.map(lambda entry: (entry.i, (entry.j, entry.value)))\n",
    "    product_entries = m.join(n)\\\n",
    "    .map(lambda tup: ((tup[1][0][0], tup[1][1][0]), (tup[1][0][1] * tup[1][1][1])))\\\n",
    "    .reduceByKey(add)\\\n",
    "    .map(lambda record: MatrixEntry(record[0][0], record[0][1], record[1]))\n",
    "    \n",
    "    return pyspark.mllib.linalg.distributed.CoordinateMatrix(product_entries)\n",
    "\n",
    "def coordinateMatrixAdd(leftmat, rightmat, scalar):\n",
    "    \"\"\"\n",
    "    Return leftmat + scalar * rightmat\n",
    "    \"\"\"\n",
    "    m = leftmat.entries.map(lambda entry: ((entry.i, entry.j), entry.value))\n",
    "    n = rightmat.entries.map(lambda entry: ((entry.i, entry.j), scalar * entry.value))\n",
    "    matsum = m.fullOuterJoin(n)\\\n",
    "    .map(lambda tup: MatrixEntry(tup[0][0], tup[0][1],\n",
    "                                 reduce(add, filter(lambda elt: elt is not None, tup[1]))))\n",
    "    \n",
    "    #return matsum\n",
    "    return pyspark.mllib.linalg.distributed.CoordinateMatrix(matsum)\n",
    "\n",
    "def coordinate_matrix_elementwise_vector_division(mat, vec):\n",
    "    \"\"\"\n",
    "    mat : CoordinateMatrix\n",
    "    \n",
    "    mat_{ij} -> mat_{ij}/vec_{i}\n",
    "    \"\"\"\n",
    "    m = mat.entries.map(lambda entry: (entry.i, (entry.j, entry.value)))\n",
    "    v = vec.entries.map(lambda entry: (entry.i, (entry.j, entry.value)))\n",
    "    matdiv = m.join(v).map(lambda tup: MatrixEntry(tup[0], tup[1][0][0], float(tup[1][0][1]) / tup[1][1][1]))\n",
    "    \n",
    "    return pyspark.mllib.linalg.distributed.CoordinateMatrix(matdiv)\n",
    "\n",
    "def coordinate_matrix_elementwise_matrix_multiplication(mat1, mat2):\n",
    "    \"\"\"\n",
    "    mat : CoordinateMatrix\n",
    "    \n",
    "    return matprod, where matprod_{ij} = mat1_{ij} * mat2_{ij}\n",
    "    \"\"\"\n",
    "    m1 = mat1.entries.map(lambda entry: ((entry.i, entry.j), entry.value))\n",
    "    m2 = mat2.entries.map(lambda entry: ((entry.i, entry.j), entry.value))\n",
    "    \n",
    "    matprod = m1.join(m2).map(lambda tup: MatrixEntry(tup[0][0], tup[0][1], tup[1][0] * tup[1][1]))\n",
    "    return pyspark.mllib.linalg.distributed.CoordinateMatrix(matprod)\n",
    "\n",
    "def coordinate_matrix_sumj(mat):\n",
    "    \"\"\"\n",
    "    mat : CoordinateMatrix\n",
    "    \"\"\"\n",
    "    summed_entries = mat.entries.map(lambda entry: (entry.i, entry.value)).reduceByKey(add)\\\n",
    "                          .map(lambda tup: MatrixEntry(tup[0], 0, tup[1]))\n",
    "\n",
    "    return pyspark.mllib.linalg.distributed.CoordinateMatrix(summed_entries)\n",
    "\n",
    "def coordinate_matrix_row(mat, i):\n",
    "    \"\"\"\n",
    "    mat : CoordinateMatrix\n",
    "    \n",
    "    return the specified row vector\n",
    "    \"\"\"\n",
    "    filtered_entries = mat.entries.filter(lambda entry: entry.i == i).\\\n",
    "        map(lambda entry: MatrixEntry(0, entry.j, entry.value))\n",
    "    return pyspark.mllib.linalg.distributed.CoordinateMatrix(filtered_entries)\n",
    "\n",
    "def coordinate_vector_matrix_norm(vec):\n",
    "    \"\"\"\n",
    "    TODO: type checking, confusing name?\n",
    "    \"\"\"\n",
    "    return np.sqrt(vec.entries.map(lambda entry: entry.value**2).sum())\n",
    "\n",
    "def coordinate_matrix_vector_l2(vec1, vec2):\n",
    "    \"\"\"\n",
    "    Given two vectors of the data type CoordinateMatrix, return L2 norm of vec1/|vec1| - vec2/|vec2|\n",
    "    \"\"\"\n",
    "    norm1, norm2 = map(coordinate_vector_matrix_norm, [vec1, vec2])\n",
    "\n",
    "    vec1normed = coordinateMatrixScalarMult(vec1, 1./norm1)\n",
    "    vec2normed = coordinateMatrixScalarMult(vec2, 1./norm2)\n",
    "    \n",
    "    diff = coordinateMatrixAdd(vec1normed, vec2normed, -1.)\n",
    "    return coordinate_vector_matrix_norm(diff)\n",
    "\n",
    "def sort_row_indices_by_distance(mat, vec):\n",
    "    \"\"\"\n",
    "    Return a list of the row indices of mat sorted by the ascending L2 distance between normalized row vectors and vec/|vec|\n",
    "    \"\"\"\n",
    "    size = mat.numCols()\n",
    "    row_vectors = mat.entries.map(lambda entry: (entry.i, [(entry.j, entry.value)]))\\\n",
    "        .reduceByKey(add).map(lambda tup: (tup[0], Vectors.sparse(size, tup[1])))\n",
    "    # TODO replace all 1D CoordinateMatrix instances by local sparse vectors\n",
    "    compare_vector = vec.entries.map(lambda entry: ('', [(entry.j, entry.value)])).reduceByKey(add)\\\n",
    "        .map(lambda tup: Vectors.sparse(size, tup[1])).collect()[0]\n",
    "    # TODO check vector normalization\n",
    "    return row_vectors\\\n",
    "        .map(lambda tup: (tup[1].squared_distance(compare_vector)/(tup[1].norm(2) * compare_vector.norm(2)), tup[0])).sortByKey()\\\n",
    "        .map(lambda tup: tup[1]).collect()\n",
    "        \n",
    "def coordinatematrix_get_row(mat, i):\n",
    "    return pyspark.mllib.linalg.distributed.CoordinateMatrix(\n",
    "                    mat.entries.filter(lambda entry: entry.i == i))\n",
    "\n",
    "def coordinatematrix_to_sparse_vector(mat):\n",
    "    \"\"\"\n",
    "    mat : CoordinateMatrix\n",
    "    \n",
    "    Mat is assumed to have non-zero entries in only the 0th row index\n",
    "    \"\"\"\n",
    "    size = mat.numCols()\n",
    "    return Vectors.sparse(size, mat.entries.map(lambda entry: (entry.j, entry.value)).collect())\n",
    "\n",
    "def coordinatematrix_multiply_vector_elementwise(mat, vec):\n",
    "    \"\"\"\n",
    "    mat : CoordinateMatrix\n",
    "    vec : CoordinateMatrix\n",
    "    \"\"\"\n",
    "    mat_entries = mat.entries.map(lambda entry: (entry.i, (entry.j, entry.value)))\n",
    "    vec_entries = vec.entries.map(lambda entry: (entry.i, entry.value))\n",
    "\n",
    "    prod = vec_entries.join(mat_entries).map(lambda tup: MatrixEntry(tup[0], tup[1][1][0], tup[1][0] * tup[1][1][1]))\n",
    "    return pyspark.mllib.linalg.distributed.CoordinateMatrix(prod)\n",
    "\n",
    "\n",
    "def coordinateMatrixElementwise(mat, op):\n",
    "    \"\"\"\n",
    "    elt -> op(elt) for each nonzero element elt of the matrix mat\n",
    "    \"\"\"\n",
    "    new_entries = mat.entries.map(lambda entry: MatrixEntry(entry.i, entry.j, op(entry.value)))\n",
    "    return pyspark.mllib.linalg.distributed.CoordinateMatrix(new_entries)\n",
    "\n",
    "\n",
    "def coordinateMatrixElementwiseMultiplication(mat, scalar):\n",
    "    \"\"\"\n",
    "    return scalar * mat\n",
    "    \"\"\"\n",
    "    new_entries = mat.entries.map(lambda entry: MatrixEntry(entry.i, entry.j, scalar * entry.value))\n",
    "    return pyspark.mllib.linalg.distributed.CoordinateMatrix(new_entries)\n",
    "\n",
    "def coordinate_matrix_to_ndarr(mat):\n",
    "    size = mat.entries.count()\n",
    "    elts = mat.entries.take(size)\n",
    "    arr = np.zeros((mat.numRows(), mat.numCols()))\n",
    "    for elt in elts:\n",
    "        arr[elt.i][elt.j] = elt.value\n",
    "    return arr\n",
    "\n",
    "def ndarr_to_coord_array(arr):\n",
    "    entries = []\n",
    "    for i in range(len(arr)):\n",
    "        for j in range(len(arr[0])):\n",
    "            if arr[i][j] != 0:\n",
    "                entries.append((i, j, arr[i][j]))\n",
    "    return CoordinateMatrix(sc.parallelize(entries))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
